{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/keras-pretrained-imagenet-weights/image_classifiers-1.0.0-py3-none-any.whl\n\nfrom classification_models.tfkeras import Classifiers\nClassifiers.models_names()","metadata":{"papermill":{"duration":14.536165,"end_time":"2021-02-08T01:26:40.845381","exception":false,"start_time":"2021-02-08T01:26:26.309216","status":"completed"},"tags":[],"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-23T18:04:41.879366Z","iopub.execute_input":"2021-08-23T18:04:41.8798Z","iopub.status.idle":"2021-08-23T18:04:58.044488Z","shell.execute_reply.started":"2021-08-23T18:04:41.879697Z","shell.execute_reply":"2021-08-23T18:04:58.043356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ML tools \nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import Adam\nfrom tensorflow.keras import Model\n# import tensorflow.keras.applications.efficientnet as efn\nfrom tensorflow.keras.applications import *\nimport os\nfrom keras import optimizers\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.776,"end_time":"2021-02-08T01:26:41.641342","exception":false,"start_time":"2021-02-08T01:26:40.865342","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:09:18.375688Z","iopub.execute_input":"2021-08-23T18:09:18.376174Z","iopub.status.idle":"2021-08-23T18:09:19.169031Z","shell.execute_reply.started":"2021-08-23T18:09:18.376124Z","shell.execute_reply":"2021-08-23T18:09:19.167921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/nih-dataframe/NIH_Dataframe.csv')\ndf.img_ind= df.img_ind.apply(lambda x: x.split('.')[0])\ndisplay(df.head(4))\nprint(df.shape)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","papermill":{"duration":0.237485,"end_time":"2021-02-08T01:26:41.898073","exception":false,"start_time":"2021-02-08T01:26:41.660588","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:09:19.170684Z","iopub.execute_input":"2021-08-23T18:09:19.171139Z","iopub.status.idle":"2021-08-23T18:09:19.476469Z","shell.execute_reply.started":"2021-08-23T18:09:19.171099Z","shell.execute_reply":"2021-08-23T18:09:19.475176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_cols = df.drop(['img_ind'], axis=1).columns.to_list()\nn_classes = len(target_cols)\nimg_size = 600\nn_epochs = 35\nlr= 0.0001\nseed= 11\nval_split= 0.2\nseed= 33\nbatch_size=12\nn_classes","metadata":{"papermill":{"duration":0.027297,"end_time":"2021-02-08T01:26:42.266735","exception":false,"start_time":"2021-02-08T01:26:42.239438","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:09:19.560314Z","iopub.execute_input":"2021-08-23T18:09:19.560701Z","iopub.status.idle":"2021-08-23T18:09:19.576097Z","shell.execute_reply.started":"2021-08-23T18:09:19.560669Z","shell.execute_reply":"2021-08-23T18:09:19.574339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return strategy\n\n'''\nReference\nhttps://www.kaggle.com/xhlulu/ranzcr-efficientnet-tpu-training\n\n'''\n\ndef build_decoder(with_labels=True, target_size=(img_size, img_size), ext='jpg'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path) # Reads and outputs the entire contents of the input filename.\n\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels=3) # Decode a PNG-encoded image to a uint8 or uint16 tensor\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels=3) # Decode a JPEG-encoded image to a uint8 tensor\n        else:\n            raise ValueError(\"Image extension not supported\")\n\n        img = tf.cast(img, tf.float32) / 255.0 # Casts a tensor to the type float32 and divides by 255.\n        img = tf.image.resize(img, target_size) # Resizing to target size\n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), label\n    \n    return decode_with_labels if with_labels else decode\n\n\ndef build_augmenter(with_labels=True):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, 0.8, 1.2)\n        img = tf.image.random_brightness(img, 0.1)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        return img\n    \n    def augment_with_labels(img, label):\n        return augment(img), label\n    \n    return augment_with_labels if with_labels else augment\n\ndef build_dataset(paths, labels=None, bsize=32, cache=True,\n                  decode_fn=None, augment_fn=None,\n                  augment=True, repeat=True, shuffle=1024, \n                  cache_dir=\"\"):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n    \n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = paths if labels is None else (paths, labels)\n    \n    dset = tf.data.Dataset.from_tensor_slices(slices)\n    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n    dset = dset.cache(cache_dir) if cache else dset\n    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n    dset = dset.repeat() if repeat else dset\n    dset = dset.shuffle(shuffle) if shuffle else dset\n    dset = dset.batch(bsize).prefetch(AUTO) # overlaps data preprocessing and model execution while training\n    return dset\n","metadata":{"papermill":{"duration":0.041826,"end_time":"2021-02-08T01:26:42.328543","exception":false,"start_time":"2021-02-08T01:26:42.286717","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:09:20.072815Z","iopub.execute_input":"2021-08-23T18:09:20.073194Z","iopub.status.idle":"2021-08-23T18:09:20.094225Z","shell.execute_reply.started":"2021-08-23T18:09:20.073162Z","shell.execute_reply":"2021-08-23T18:09:20.092511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET_NAME = \"nih-image-600x600-data\"\nstrategy = auto_select_accelerator()\nbatch_size = strategy.num_replicas_in_sync * batch_size\nprint('batch size', batch_size)","metadata":{"papermill":{"duration":6.314494,"end_time":"2021-02-08T01:26:48.663271","exception":false,"start_time":"2021-02-08T01:26:42.348777","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:09:20.49499Z","iopub.execute_input":"2021-08-23T18:09:20.495358Z","iopub.status.idle":"2021-08-23T18:09:25.697564Z","shell.execute_reply.started":"2021-08-23T18:09:20.495326Z","shell.execute_reply":"2021-08-23T18:09:25.696067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path(DATASET_NAME)\nGCS_DS_PATH","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:09:25.699541Z","iopub.execute_input":"2021-08-23T18:09:25.700007Z","iopub.status.idle":"2021-08-23T18:09:26.223611Z","shell.execute_reply.started":"2021-08-23T18:09:25.699961Z","shell.execute_reply":"2021-08-23T18:09:26.222474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths = GCS_DS_PATH + \"/NIH_Images/\" + df['img_ind'] + '.jpg'\n\n\n#Get the multi-labels\nlabel_cols = df.columns[:-1]\nlabels = df[label_cols].values","metadata":{"papermill":{"duration":0.11783,"end_time":"2021-02-08T01:26:48.801455","exception":false,"start_time":"2021-02-08T01:26:48.683625","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:09:26.225542Z","iopub.execute_input":"2021-08-23T18:09:26.225981Z","iopub.status.idle":"2021-08-23T18:09:26.280877Z","shell.execute_reply.started":"2021-08-23T18:09:26.225933Z","shell.execute_reply":"2021-08-23T18:09:26.279243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Class Weights","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import class_weight\nprint('    Total: ', labels.shape[0])\nfor disease in range(labels.shape[1]):\n    neg, pos = np.bincount(labels[:, disease].astype('int64'))\n    print(f'Disease {disease}:\\n    Positive: {pos} ({100 * pos / (neg+pos):.2f}% of total)\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:11:03.675792Z","iopub.execute_input":"2021-08-23T18:11:03.676152Z","iopub.status.idle":"2021-08-23T18:11:03.69162Z","shell.execute_reply.started":"2021-08-23T18:11:03.676122Z","shell.execute_reply":"2021-08-23T18:11:03.690658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\ndef generate_class_weights(class_series, multi_class=True, one_hot_encoded=False):\n    if multi_class:\n    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n        if one_hot_encoded:\n            class_series = np.argmax(class_series, axis=1)\n  \n        # Compute class weights with sklearn method\n        class_labels = np.unique(class_series)\n        class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n        return dict(zip(class_labels, class_weights))\n    else:\n        # It is neccessary that the multi-label values are one-hot encoded\n        mlb = None\n        if not one_hot_encoded:\n            mlb = MultiLabelBinarizer()\n            class_series = mlb.fit_transform(class_series)\n\n        n_samples = len(class_series)\n        n_classes = len(class_series[0])\n\n        # Count each class frequency\n        class_count = [0] * n_classes\n        for classes in class_series:\n            for index in range(n_classes):\n                if classes[index] != 0:\n                    class_count[index] += 1\n    \n    # Compute class weights using balanced method\n    class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\n    class_labels = range(len(class_weights)) if mlb is None else mlb.classes_\n    return dict(zip(class_labels, class_weights))\n\nclass_weights = generate_class_weights(labels, one_hot_encoded=True)\nclass_weights","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:11:04.740145Z","iopub.execute_input":"2021-08-23T18:11:04.74051Z","iopub.status.idle":"2021-08-23T18:11:04.782402Z","shell.execute_reply.started":"2021-08-23T18:11:04.74048Z","shell.execute_reply":"2021-08-23T18:11:04.781443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split","metadata":{}},{"cell_type":"code","source":"# Train test split\n(train_paths, valid_paths, \n  train_labels, valid_labels) = train_test_split(paths, labels, test_size=val_split, random_state=11)\n\nprint(train_paths.shape, valid_paths.shape)\ntrain_labels.sum(axis=0), valid_labels.sum(axis=0)","metadata":{"papermill":{"duration":0.056789,"end_time":"2021-02-08T01:26:48.878703","exception":false,"start_time":"2021-02-08T01:26:48.821914","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:11:10.430289Z","iopub.execute_input":"2021-08-23T18:11:10.430642Z","iopub.status.idle":"2021-08-23T18:11:10.454959Z","shell.execute_reply.started":"2021-08-23T18:11:10.430612Z","shell.execute_reply":"2021-08-23T18:11:10.453877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the tensorflow datasets\n\ndecoder = build_decoder(with_labels=True, target_size=(img_size, img_size))\n\n# Build the tensorflow datasets\ndtrain = build_dataset(\n    train_paths, train_labels, bsize=batch_size, decode_fn=decoder\n)\n\ndvalid = build_dataset(\n    valid_paths, valid_labels, bsize=batch_size, \n    repeat=False, shuffle=False, augment=False, decode_fn=decoder\n)","metadata":{"papermill":{"duration":0.372349,"end_time":"2021-02-08T01:26:49.431926","exception":false,"start_time":"2021-02-08T01:26:49.059577","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:11:11.390179Z","iopub.execute_input":"2021-08-23T18:11:11.39059Z","iopub.status.idle":"2021-08-23T18:11:11.787226Z","shell.execute_reply.started":"2021-08-23T18:11:11.390557Z","shell.execute_reply":"2021-08-23T18:11:11.786158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Show random samples","metadata":{}},{"cell_type":"code","source":"data, _ = dtrain.take(2)\nimages = data[0].numpy()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:11:12.404922Z","iopub.execute_input":"2021-08-23T18:11:12.405302Z","iopub.status.idle":"2021-08-23T18:12:09.629155Z","shell.execute_reply.started":"2021-08-23T18:11:12.405268Z","shell.execute_reply":"2021-08-23T18:12:09.628344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 4, figsize=(20,10))\naxes = axes.flatten()\nfor img, ax in zip(images, axes):\n    ax.imshow(img)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:49:03.872023Z","iopub.execute_input":"2021-08-23T14:49:03.872294Z","iopub.status.idle":"2021-08-23T14:49:05.785182Z","shell.execute_reply.started":"2021-08-23T14:49:03.872268Z","shell.execute_reply":"2021-08-23T14:49:05.784194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"def build_model():\n    seresnet152, _ = Classifiers.get('seresnet152')\n    \n    inp = layers.Input(shape = (img_size, img_size, 3))\n    pretrained_base = seresnet152(input_shape=(img_size, img_size, 3), \n                       include_top=False, \n                       input_tensor = inp,\n                       weights='imagenet')\n    conv2d_254 = pretrained_base.get_layer('conv2d_254')\n    x = layers.Dropout(0.16)(conv2d_254.output)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(n_classes, 'sigmoid')(x)\n    return Model(inp, x)","metadata":{"papermill":{"duration":0.033245,"end_time":"2021-02-08T01:26:49.48767","exception":false,"start_time":"2021-02-08T01:26:49.454425","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:14:02.526376Z","iopub.execute_input":"2021-08-23T18:14:02.527159Z","iopub.status.idle":"2021-08-23T18:14:02.536732Z","shell.execute_reply.started":"2021-08-23T18:14:02.527106Z","shell.execute_reply":"2021-08-23T18:14:02.535615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model= build_model()\n    loss= tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0)\n    model.compile(optimizers.Adam(lr=lr),loss=loss,metrics=[tf.keras.metrics.AUC(multi_label=True)])","metadata":{"papermill":{"duration":122.352746,"end_time":"2021-02-08T01:28:51.862611","exception":false,"start_time":"2021-02-08T01:26:49.509865","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:14:04.749956Z","iopub.execute_input":"2021-08-23T18:14:04.750331Z","iopub.status.idle":"2021-08-23T18:14:45.750206Z","shell.execute_reply.started":"2021-08-23T18:14:04.750298Z","shell.execute_reply":"2021-08-23T18:14:45.749132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T18:14:45.751689Z","iopub.execute_input":"2021-08-23T18:14:45.752106Z","iopub.status.idle":"2021-08-23T18:14:46.191871Z","shell.execute_reply.started":"2021-08-23T18:14:45.752071Z","shell.execute_reply":"2021-08-23T18:14:46.190526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name= 'NIH_Seresnet152_model.h5'\n\nrlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 3, verbose = 1, \n                                min_delta = 1e-4, min_lr = 1e-6, mode = 'min', cooldown=1)\n        \nckp = ModelCheckpoint(name,monitor = 'val_loss',\n                      verbose = 1, save_best_only = True, mode = 'min')\n        \nes = EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 5, mode = 'min', \n                    restore_best_weights = True, verbose = 1)","metadata":{"papermill":{"duration":0.060939,"end_time":"2021-02-08T01:28:52.216709","exception":false,"start_time":"2021-02-08T01:28:52.15577","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:16:07.489613Z","iopub.execute_input":"2021-08-23T18:16:07.490808Z","iopub.status.idle":"2021-08-23T18:16:07.50064Z","shell.execute_reply.started":"2021-08-23T18:16:07.490726Z","shell.execute_reply":"2021-08-23T18:16:07.499248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps_per_epoch = (train_paths.shape[0] // batch_size)\nsteps_per_epoch","metadata":{"papermill":{"duration":0.054513,"end_time":"2021-02-08T01:28:52.313072","exception":false,"start_time":"2021-02-08T01:28:52.258559","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:16:08.090164Z","iopub.execute_input":"2021-08-23T18:16:08.090554Z","iopub.status.idle":"2021-08-23T18:16:08.097604Z","shell.execute_reply.started":"2021-08-23T18:16:08.090523Z","shell.execute_reply":"2021-08-23T18:16:08.096574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(dtrain,\n                    validation_data=dvalid,\n                    epochs=n_epochs,\n                    callbacks=[rlr,es,ckp],\n                    steps_per_epoch=steps_per_epoch,\n                    verbose=1)","metadata":{"papermill":{"duration":4545.439448,"end_time":"2021-02-08T02:44:37.790427","exception":false,"start_time":"2021-02-08T01:28:52.350979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T18:16:09.485281Z","iopub.execute_input":"2021-08-23T18:16:09.485682Z","iopub.status.idle":"2021-08-23T20:11:37.433026Z","shell.execute_reply.started":"2021-08-23T18:16:09.485648Z","shell.execute_reply":"2021-08-23T20:11:37.430887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('full_multi_model.h5')","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:11:37.43588Z","iopub.execute_input":"2021-08-23T20:11:37.436458Z","iopub.status.idle":"2021-08-23T20:11:50.785204Z","shell.execute_reply.started":"2021-08-23T20:11:37.436385Z","shell.execute_reply":"2021-08-23T20:11:50.782772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"pred = model.predict(dvalid, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:11:50.789521Z","iopub.execute_input":"2021-08-23T20:11:50.789948Z","iopub.status.idle":"2021-08-23T20:12:50.545351Z","shell.execute_reply.started":"2021-08-23T20:11:50.789907Z","shell.execute_reply":"2021-08-23T20:12:50.544085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 6))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.plot( history.history[\"loss\"], label = \"Training Loss\", marker='o')\nplt.plot( history.history[\"val_loss\"], label = \"Validation Loss\", marker='+')\nplt.grid(True)\nplt.legend()\nplt.show()","metadata":{"papermill":{"duration":0.74803,"end_time":"2021-02-08T02:44:39.033205","exception":false,"start_time":"2021-02-08T02:44:38.285175","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T20:12:50.548396Z","iopub.execute_input":"2021-08-23T20:12:50.5492Z","iopub.status.idle":"2021-08-23T20:12:50.852528Z","shell.execute_reply.started":"2021-08-23T20:12:50.549101Z","shell.execute_reply":"2021-08-23T20:12:50.851066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 6))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"AUC\")\nplt.plot( history.history[\"auc\"], label = \"Training AUC\" , marker='o')\nplt.plot( history.history[\"val_auc\"], label = \"Validation AUC\", marker='+')\nplt.grid(True)\nplt.legend()\nplt.show()","metadata":{"papermill":{"duration":0.693171,"end_time":"2021-02-08T02:44:40.279118","exception":false,"start_time":"2021-02-08T02:44:39.585947","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T20:12:50.854006Z","iopub.execute_input":"2021-08-23T20:12:50.854389Z","iopub.status.idle":"2021-08-23T20:12:51.083925Z","shell.execute_reply.started":"2021-08-23T20:12:50.854351Z","shell.execute_reply":"2021-08-23T20:12:51.081757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\n#model= tf.keras.models.load_model(name)\n#pred= model.predict(dvalid, verbose=1)\n\nprint('AUC and Accuracy CKECK-UP per CLASS')\n\nclasses= df.columns[:-1]\nfor i, n in enumerate(classes):\n    print(classes[i])\n    print('AUC: ', roc_auc_score(valid_labels[:, i], pred[:, i]))\n    print('Accuracy:', accuracy_score(valid_labels[:, i], (pred[:, i]>0.5).astype(int)))\n    print('---------')","metadata":{"papermill":{"duration":501.673242,"end_time":"2021-02-08T02:53:02.443459","exception":false,"start_time":"2021-02-08T02:44:40.770217","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-23T20:12:51.086172Z","iopub.execute_input":"2021-08-23T20:12:51.086703Z","iopub.status.idle":"2021-08-23T20:12:51.238869Z","shell.execute_reply.started":"2021-08-23T20:12:51.086656Z","shell.execute_reply":"2021-08-23T20:12:51.23746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explainability","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Display\nfrom IPython.display import Image, display\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:12:51.240283Z","iopub.execute_input":"2021-08-23T20:12:51.240627Z","iopub.status.idle":"2021-08-23T20:12:51.247892Z","shell.execute_reply.started":"2021-08-23T20:12:51.240592Z","shell.execute_reply":"2021-08-23T20:12:51.246034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_path = '../input/data/images_003/images/00003923_015.png'","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:12:51.250762Z","iopub.execute_input":"2021-08-23T20:12:51.251148Z","iopub.status.idle":"2021-08-23T20:12:51.262102Z","shell.execute_reply.started":"2021-08-23T20:12:51.251115Z","shell.execute_reply":"2021-08-23T20:12:51.260619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimg = cv2.imread(img_path)\n#image size\nsize=(1,600,600,3)\n#resize image\nimg.resize(size)\n\npreds = model.predict(img)\n\nprint(preds)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T20:12:51.263933Z","iopub.execute_input":"2021-08-23T20:12:51.264375Z","iopub.status.idle":"2021-08-23T20:12:52.539389Z","shell.execute_reply.started":"2021-08-23T20:12:51.264339Z","shell.execute_reply":"2021-08-23T20:12:52.537189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\nimport tensorflow.keras as K\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize\nfrom tensorflow.keras.models import Model\n\n\ndef VizGradCAM(model, image, interpolant=0.5, plot_results=True):\n    \"\"\"VizGradCAM - Displays GradCAM based on Keras / TensorFlow models\n    using the gradients from the last convolutional layer. This function\n    should work with all Keras Application listed here:\n    https://keras.io/api/applications/\n    Parameters:\n    model (keras.model): Compiled Model with Weights Loaded\n    image: Image to Perform Inference On\n    plot_results (boolean): True - Function Plots using PLT\n                            False - Returns Heatmap Array\n    Returns:\n    Heatmap Array?\n    \"\"\"\n    # Sanity Check\n    assert (\n        interpolant > 0 and interpolant < 1\n    ), \"Heatmap Interpolation Must Be Between 0 - 1\"\n\n    last_conv_layer = next(\n        x for x in model.layers[::-1] if isinstance(x, K.layers.Conv2D)\n    )\n    target_layer = model.get_layer(last_conv_layer.name)\n\n    original_img = image\n    img = np.expand_dims(original_img, axis=0)\n    prediction = model.predict(img)\n\n    # Obtain Prediction Index\n    prediction_idx = np.argmax(prediction)\n\n    # Compute Gradient of Top Predicted Class\n    with tf.GradientTape() as tape:\n        gradient_model = Model([model.inputs], [target_layer.output, model.output])\n        conv2d_out, prediction = gradient_model(img)\n        # Obtain the Prediction Loss\n        loss = prediction[:, prediction_idx]\n\n    # Gradient() computes the gradient using operations recorded\n    # in context of this tape\n    gradients = tape.gradient(loss, conv2d_out)\n\n    # Obtain the Output from Shape [1 x H x W x CHANNEL] -> [H x W x CHANNEL]\n    output = conv2d_out[0]\n\n    # Obtain Depthwise Mean\n    weights = tf.reduce_mean(gradients[0], axis=(0, 1))\n\n    # Create a 7x7 Map for Aggregation\n    activation_map = np.zeros(output.shape[0:2], dtype=np.float32)\n\n    # Multiply Weights with Every Layer\n    for idx, weight in enumerate(weights):\n        activation_map += weight * output[:, :, idx]\n\n    # Resize to Size of Image\n    activation_map = cv2.resize(\n        activation_map.numpy(), (original_img.shape[1], original_img.shape[0])\n    )\n\n    # Ensure No Negative Numbers\n    activation_map = np.maximum(activation_map, 0)\n\n    # Convert Class Activation Map to 0 - 255\n    activation_map = (activation_map - activation_map.min()) / (\n        activation_map.max() - activation_map.min() + .00001\n    )\n    activation_map = np.uint8(255 * activation_map)\n\n    # Convert to Heatmap\n    heatmap = cv2.applyColorMap(activation_map, cv2.COLORMAP_JET)\n\n    # Superimpose Heatmap on Image Data\n    original_img = np.uint8(\n        (original_img - original_img.min())\n        / (original_img.max() - original_img.min() + .0001)\n        * 255\n    )\n\n    cvt_heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n\n    # Enlarge Plot\n    plt.rcParams[\"figure.dpi\"] = 100\n\n    if plot_results == True:\n        plt.imshow(\n            np.uint8(original_img * interpolant + cvt_heatmap * (1 - interpolant))\n        )\n        \n        plt.savefig('multi.png')\n    else:\n        return cvt_heatmap","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import load_img, img_to_array\n\n# Load Your Favourite Image\ntest_img = img_to_array(load_img(img_path , target_size=(600,600)))\n\n# Use The Function - Boom!\nVizGradCAM(model, test_img)","metadata":{},"execution_count":null,"outputs":[]}]}